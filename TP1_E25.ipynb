{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfEnYLlo9yAM"
   },
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP1 ETE 2025 - Préparation de données\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aveyr1en994y"
   },
   "source": [
    "### Instructions de remise\n",
    "\n",
    "#### Membres de l'équipe :\n",
    "    - Mathieu Brulotte (2144294) 1\n",
    "    - Nom (Matricule) 2\n",
    "    - Nom (Matricule) 3\n",
    "\n",
    "#### Numéro du groupe :\n",
    "    - TP - Groupe #8\n",
    "    \n",
    "#### Livrable :\n",
    "\n",
    "Vous devez soumettre ce notebook sur Moodle dans la boite de remise sous le nom TP1_NumeroGroupe_matricule1_matricule2_matricule3.ipynb.\n",
    "\n",
    "**NB**: Tout travail en retard sera pénalisé d'une valeur de 10\\% par jour de retard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WFBkYc660pI"
   },
   "source": [
    "## Introduction et objectifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fbCQ4YHNFtn"
   },
   "source": [
    "### Importation des différents modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dni6R3K760pK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Collecting numpy!=1.24.0,>=1.20 (from seaborn)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy, seaborn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed numpy-1.26.4 seaborn-0.13.2\n",
      "Requirement already satisfied: matplotlib in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-1.39.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from plotly) (24.1)\n",
      "Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 4.5/14.8 MB 24.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.2/14.8 MB 23.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.8 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.8/14.8 MB 22.7 MB/s eta 0:00:00\n",
      "Downloading narwhals-1.39.0-py3-none-any.whl (339 kB)\n",
      "Installing collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-1.39.0 plotly-6.0.1\n",
      "Collecting shap\n",
      "  Downloading shap-0.47.2-cp312-cp312-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from shap) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from shap) (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from shap) (4.66.5)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from shap) (24.1)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Downloading numba-0.61.2-cp312-cp312-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from shap) (4.12.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.54->shap)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\logs\\conda\\envs\\a2_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.47.2-cp312-cp312-win_amd64.whl (545 kB)\n",
      "   ---------------------------------------- 0.0/545.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 545.2/545.2 kB 18.9 MB/s eta 0:00:00\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading numba-0.61.2-cp312-cp312-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 16.5 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading llvmlite-0.44.0-cp312-cp312-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 5.0/30.3 MB 23.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 10.0/30.3 MB 23.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 14.7/30.3 MB 23.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 20.2/30.3 MB 24.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 25.2/30.3 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.1/30.3 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 24.1 MB/s eta 0:00:00\n",
      "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.1.1 llvmlite-0.44.0 numba-0.61.2 shap-0.47.2 slicer-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install plotly\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "beJjTQOt60pM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Logs\\Conda\\envs\\A2_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn import linear_model\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe5Ms4DINc8u"
   },
   "source": [
    "### Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m_QcX1llNT2d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txn5hmVWBSzF"
   },
   "source": [
    "Le but de ce notebook est d'effectuer le prétraitement du dataset [HousePricePrediction](https://docs.google.com/spreadsheets/d/1caaR9pT24GNmq3rDQpMiIMJrmiTGarbs/edit#gid=1150341366) qui pourra être par la suite être utilisé pour entraîner un modèle de prédiction de prix de maisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyYKhZfI60pN"
   },
   "source": [
    "## Préparation des données\n",
    "\n",
    "Plusieurs étapes sont nécessaires pour préparer un dataset pour la fouille des données\n",
    "- **Nettoyage des données** :\n",
    "    - Suppression des attributs inutiles\n",
    "    - Gestion des valeurs manquantes\n",
    "    - Gestion des valeurs aberrantes\n",
    "- **Transformation des données** :\n",
    "    - Encodage des données\n",
    "    - Normalisation des données\n",
    "- **Sélection des attributs** :\n",
    "    - Suppression des attributs les plus fortement corrélés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SXXhu9S60pO"
   },
   "source": [
    "<a id=\"exploration-des-donnees\"></a>\n",
    "## 1. Exploration des données (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPtQqe4J60pO"
   },
   "source": [
    "Nous vous avons fourni le fichier *data.csv* avec l'exécution de la deuxième cellule. Il contient l'ensemble des données. Chaque ligne contient les données d'une vente. La description des attributs du dataset est la suivante:\n",
    "\n",
    "| # | Feature Name | Description |\n",
    "|---|--------------|-------------|\n",
    "| 1 | Id           | Numéro de vente / To count the records. |\n",
    "| 2 | MSSubClass   | Type de logement / Identifies the type of dwelling involved in the sale. |\n",
    "| 3 | MSZoning     | Zonage / Identifies the general zoning classification of the sale. |\n",
    "| 4 | LotArea      | Superficie du logement / Lot size in square feet. |\n",
    "| 5 | LotConfig    | Configuration du logement / Configuration of the lot |\n",
    "| 6 | BldgType     | Type de logement / Type of dwelling |\n",
    "| 7 | OverallCond  | Etat général / Rates the overall condition of the house |\n",
    "| 8 | YearBuilt    | Année de contruction / Original construction year |\n",
    "| 9 | YearRemodAdd | Année de rénovation / Remodel date (same as construction date if no remodeling or additions). |\n",
    "| 10| Exterior1st  | Type de revêtement extérieur / Exterior covering on house |\n",
    "| 11| BsmtFinSF2   | Surface de vie / Type 2 finished square feet. |\n",
    "| 12| TotalBsmtSF  | Surface totale de la base / Total square feet of basement area |\n",
    "| 13| SalePrice    | Prix de vente à prédire / To be predicted |\n",
    "\n",
    "On visualise le dataset pour avoir une idée de ce qu'il contient et des prétraitements à effectuer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaalN_p760pO"
   },
   "source": [
    "### 1.1 - Question 1 (2.5 points)\n",
    "\n",
    "**Combien d'éléments contient le dataset ? Quelles sont les types des attributs du dataset ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "srCKpbVl60pO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dataset contient 2919 lignes et 13 colonnes\n",
      "Les types des colonnes sont les suivantes :\n",
      "Id : int64\n",
      "MSSubClass : int64\n",
      "MSZoning : object\n",
      "LotArea : int64\n",
      "LotConfig : object\n",
      "BldgType : object\n",
      "OverallCond : int64\n",
      "YearBuilt : int64\n",
      "YearRemodAdd : int64\n",
      "Exterior1st : object\n",
      "BsmtFinSF2 : float64\n",
      "TotalBsmtSF : float64\n",
      "SalePrice : float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Le dataset contient {len(df)} lignes et {len(df.columns)} colonnes\")\n",
    "print(\"Les types des colonnes sont les suivantes :\")\n",
    "for i, j in enumerate(df.dtypes):\n",
    "    print(f\"{df.columns[i]} : {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iopxmVPF60pP"
   },
   "source": [
    "### 1.2 - Question 2 (2.5 points)\n",
    "\n",
    "**Quelles sont les valeurs uniques des attributs de type `object` ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les valeurs uniques de la colonne MSZoning\n",
      "RL         2265\n",
      "RM          460\n",
      "FV          139\n",
      "RH           26\n",
      "C (all)      25\n",
      "Name: count, dtype: int64\n",
      "Le nombre de valeurs nulles est : 4\n",
      "\n",
      "Les valeurs uniques de la colonne LotConfig\n",
      "Inside     2133\n",
      "Corner      511\n",
      "CulDSac     176\n",
      "FR2          85\n",
      "FR3          14\n",
      "Name: count, dtype: int64\n",
      "Le nombre de valeurs nulles est : 0\n",
      "\n",
      "Les valeurs uniques de la colonne BldgType\n",
      "1Fam      2425\n",
      "TwnhsE     227\n",
      "Duplex     109\n",
      "Twnhs       96\n",
      "2fmCon      62\n",
      "Name: count, dtype: int64\n",
      "Le nombre de valeurs nulles est : 0\n",
      "\n",
      "Les valeurs uniques de la colonne Exterior1st\n",
      "VinylSd    1025\n",
      "MetalSd     450\n",
      "HdBoard     442\n",
      "Wd Sdng     411\n",
      "Plywood     221\n",
      "CemntBd     126\n",
      "BrkFace      87\n",
      "WdShing      56\n",
      "AsbShng      44\n",
      "Stucco       43\n",
      "BrkComm       6\n",
      "AsphShn       2\n",
      "Stone         2\n",
      "CBlock        2\n",
      "ImStucc       1\n",
      "Name: count, dtype: int64\n",
      "Le nombre de valeurs nulles est : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    if df.dtypes[i] == 'object':\n",
    "        print(f\"Les valeurs uniques de la colonne {df[i].value_counts()}\")\n",
    "        print(f\"Le nombre de valeurs nulles est : {df[i].isnull().sum()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les valeurs uniques de la colonne MSZoning\n",
    "- RL         2265\n",
    "- RM          460\n",
    "- FV          139\n",
    "- RH           26\n",
    "- C (all)      25\n",
    "- Name: count, dtype: int64\n",
    "### Le nombre de valeurs nulles est : 4\n",
    "\n",
    "### Les valeurs uniques de la colonne LotConfig\n",
    "- Inside     2133\n",
    "- Corner      511\n",
    "- CulDSac     176\n",
    "- FR2          85\n",
    "- FR3          14\n",
    "- Name: count, dtype: int64\n",
    "### Le nombre de valeurs nulles est : 0\n",
    "\n",
    "### Les valeurs uniques de la colonne BldgType\n",
    "- 1Fam      2425\n",
    "- TwnhsE     227\n",
    "- Duplex     109\n",
    "- Twnhs       96\n",
    "- 2fmCon      62\n",
    "- Name: count, dtype: int64\n",
    "### Le nombre de valeurs nulles est : 0\n",
    "\n",
    "### Les valeurs uniques de la colonne Exterior1st\n",
    "- VinylSd    1025\n",
    "- MetalSd     450\n",
    "- HdBoard     442\n",
    "- Wd Sdng     411\n",
    "- Plywood     221\n",
    "- CemntBd     126\n",
    "- BrkFace      87\n",
    "- WdShing      56\n",
    "- AsbShng      44\n",
    "- Stucco       43\n",
    "- BrkComm       6\n",
    "- AsphShn       2\n",
    "- Stone         2\n",
    "- CBlock        2\n",
    "- ImStucc       1\n",
    "- Name: count, dtype: int64\n",
    "### Le nombre de valeurs nulles est : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft67cmEY60pQ"
   },
   "source": [
    "<a id=\"nettoyage-des-donnees\"></a>\n",
    "## 2. Nettoyage des données (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEAKIo8W60pQ"
   },
   "source": [
    "<a id=\"suppression-des-attributs-inutiles\"></a>\n",
    "### 2.1 Suppression des attributs inutiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njGJm6PHiiAK"
   },
   "source": [
    "### 2.1.1 - Question 3 (5 points)\n",
    "\n",
    "**Pourquoi on peut supprimer l'attribut `Id` dans le cas de ce TP? Effectuez cette suppression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS6YJCwI60pQ"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWPPDvJx60pQ"
   },
   "source": [
    "<a id=\"gestion-des-valeurs-manquantes\"></a>\n",
    "### 2.2 Gestion des valeurs manquantes\n",
    "\n",
    "Pour gérer les valeurs manquantes, plusieurs solutions s'offrent à nous :\n",
    "- Remplacer par la valeur la plus fréquente (le mode)\n",
    "- Remplacer par la valeur moyenne/médiane\n",
    "- Suppression des lignes contenant des valeurs manquantes\n",
    "\n",
    "Pour ce TP, nous utiliserons la dernière option car nous avons peu de valeurs manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijKA36QIhN9O"
   },
   "source": [
    "#### 2.2.1 - Question 4 (2.5 points)\n",
    "\n",
    "**Quels attributs contiennent des valeurs manquantes ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPdUgLoC60pQ"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD3p7SzPhvJr"
   },
   "source": [
    "#### 2.2.2 - Question 5 (2.5 points)\n",
    "\n",
    "On peut alors gérer les valeurs manquantes colonne par colonne. L'attribut `SalePrice` n'est pas pris en considération car les valeurs manquantes sont justement les valeurs que nous voulons prédire.\n",
    "\n",
    "**Supprimer les lignes contenant les valeurs manquantes. Implémentez la fonction `delete_missing_values` qui retire ces données**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lB8c_BHB60pS"
   },
   "outputs": [],
   "source": [
    "def delete_missing_values(dataset):\n",
    "    \"\"\"\n",
    "    This function deletes row whom a value is missing.\n",
    "\n",
    "    :param dataset: ensemble des données\n",
    "    :return:\n",
    "      dataset traitée\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOQyhy5Bzeea"
   },
   "outputs": [],
   "source": [
    "df = delete_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv7C76Pt60pR"
   },
   "source": [
    "Les données manquantes pour la colonne `SalePrice` sont celles du dataset de test. On laisse donc ces valeurs manquantes car on veut appliquer le même prétraitement sur les données de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd7i82HySOTw"
   },
   "source": [
    "### 2.2.3 - Question 6 (10 points)\n",
    "\n",
    "On veut néanmoins que les données d'entrainement suivent une distribution gaussienne.\n",
    "\n",
    "**Implémenter le fonction `plot_hist`. Cette fonction doit permettre d'afficher la distribution des valeurs de l'attribut `SalePrice` ainsi que la loi normale de même moyenne et variance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJVGJ9rm60pR"
   },
   "outputs": [],
   "source": [
    "def plot_hist(prices):\n",
    "    \"\"\"\n",
    "    Affiche la distribution du prix de vente\n",
    "\n",
    "    :param prices: ensemble des prix.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouDFr8tK60pR"
   },
   "outputs": [],
   "source": [
    "plot_hist(df['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niiWCCwX60pS"
   },
   "source": [
    "Vous devez obtenir une distribution des valeurs de `SalePrice` proches d'une distribution normale mais légèrement asymétrique. On peut alors appliquer une transformation logarithmique pour approcher d'une distribution normale symétrique.\n",
    "\n",
    "**Effectuer cette transformation sur notre ensemble de données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDakO5Is60pS"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVO6QYKK_msM"
   },
   "outputs": [],
   "source": [
    "plot_hist(df['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette copie va servir plus tard pour la question 6 (IQR)\n",
    "df_order1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv5pOxkH60pT"
   },
   "source": [
    "<a id=\"detection-des-valeurs-aberrantes\"></a>\n",
    "### 2.3 Détection des valeurs aberrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96toDgSp60pT"
   },
   "source": [
    "En pratique, la méthode de détection d'une valeur aberrante nécessite de se poser les questions suivantes:\n",
    "- Quelles valeurs seraient incohérentes pour chaque colonne ?\n",
    "- Quelles valeurs peuvent être problématiques pour l'utilisation de ces données ? Exemple: pour une régression linéaire, on préfère avoir des valeurs distribuées suivant une loi normale.\n",
    "\n",
    "Avec ces éléments, on peut:\n",
    "- Fixer des seuils de tolérance pour les valeurs aberrantes\n",
    "- Utiliser des algorithmes de détection de valeurs aberrantes (ex: clustering, IRQ, [QTest](https://plotly.com/python/v3/outlier-test/), ...)\n",
    "\n",
    "A noter que suivant les méthodes, les valeurs détectées comme aberrantes peuvent être différentes.\n",
    "\n",
    "La méthode IRQ fait l'objet d'une question, en fin de ce notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwgcO0k9Jjiw"
   },
   "source": [
    "### 2.3.1 Question 7 (10 points)\n",
    "\n",
    "Ici comme nous allons réaliser une régression linéaire, nous allons visuellement voir si certains points s'écartent largement de la droite de régression.\n",
    "\n",
    "On sait que l'on veut effectuer une régression linéaire pour prédire `SalePrice`. On peut donc visualiser les valeurs de chaque attribut en fonction de `SalePrice` pour détecter la présence de valeurs aberrantes.\n",
    "\n",
    "**Implémenter la fonction `plot_line`. Elle doit permettre de visualiser la relation entre un attribut donné et `SalePrice`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGLszIizKSYO"
   },
   "outputs": [],
   "source": [
    "def plot_line(attr):\n",
    "    \"\"\"\n",
    "    Affiche la relation entre attr et SalePrice\n",
    "\n",
    "    :param attr: attribut à comparer à SalePrice\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq_0y4OeK_Pm"
   },
   "source": [
    "**Afficher les relations de tous les attributs avec `SalePrice`. Peut-on y déceler des valeurs aberrantes ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EPDHuUJZ1a4"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À ce stade, il s'agit uniquement de détecter la présence éventuelle de valeurs aberrantes dans les données.**\n",
    "**Aucune action de traitement ou de remplacement n'est demandée pour le moment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrnapsJb60pV"
   },
   "source": [
    "## 3. Transformation des données (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZKsU-2t60pV"
   },
   "source": [
    "### 3.1 Encodage des attributs de type `object`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MElC6Gao60pV"
   },
   "source": [
    "Les attributs de type `object` étant catégoriques (voire partie 1), on peut effectuer un `one hot encoding` de ces attributs. `Pandas` permet d'effectuer cela avec la fonction `get_dummies()`. Cela nous permettra d'obtenir un dataset contenant uniquement des attributs de type `int` ou `float`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHcKIIn_xcbg"
   },
   "source": [
    "#### 3.1.1 Question 8 (5 points)\n",
    "\n",
    "**Encodez les attributs de type `object` avec un `one hot encoding`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oTBcJTT60pV"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nern4lpg60pW"
   },
   "source": [
    "### 3.2 Normalisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WharCiE560pW"
   },
   "source": [
    "Pour faciliter l'entraînement du modèle, on peut normaliser les données. `sklearn` permet d'effectuer cela avec les fonctions suivantes :\n",
    "\n",
    "*   `StandardScaler()` normalise les données en soustrayant la moyenne et en divisant par l'écart-type\n",
    "*   `MinMaxScaler()` normalise les données en les ramenant entre 0 et 1.\n",
    "\n",
    "Dans la suite de ce TP, nous utiliserons la fonction `StandardScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGl8KxkwXyC2"
   },
   "outputs": [],
   "source": [
    "# A utiliser dans la partie 5.2\n",
    "mu_sale_price = df[\"SalePrice\"].mean()\n",
    "sigma_sale_price = df[\"SalePrice\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0_AYgbsxzdY"
   },
   "source": [
    "#### 3.2.1 Question 9 (5 points)\n",
    "\n",
    "**Implémenter la fonction `normalize`. Elle doit réaliser la normalisation des données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsJz5irG60pW"
   },
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    \"\"\"\n",
    "    Normalise les données du dataset.\n",
    "\n",
    "    :param dataset: ensemble des données\n",
    "    :return:\n",
    "      dataset traitée\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2amj4-7mcPjS"
   },
   "outputs": [],
   "source": [
    "df = normalize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0AY3k-a60pX"
   },
   "source": [
    "## 4. Sélection des attributs corrélées (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QdxBuVE60pX"
   },
   "source": [
    "### 4.1 Suppression des attributs corrélées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqQZacRb60pX"
   },
   "source": [
    "Pour améliorer la qualité de la prédiction, nous devons prendre en compte la corrélation entre attributs. L'objectif est donc de supprimer les attributs les plus fortement corrélées entre eux.\n",
    "\n",
    "Pour ce faire, vous disposez des fonctions suivantes\n",
    "\n",
    "* `corr()` de `Pandas` qui calcule la matrice de corrélation\n",
    "* `heatmap()` de `seaborn` qui permet de visualiser la matrice de corrélation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_vrfq-hyJdL"
   },
   "source": [
    "#### 4.1.1 Question 10 (10 points)\n",
    "\n",
    "**Implémenter la fonction `display_corr_matrix`. Elle doit permettre d'afficher la matrice de corrélation entre les différents attributs de nos données après normalisation des données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAM9uYXW60pX"
   },
   "outputs": [],
   "source": [
    "def display_corr_matrix(dataset)\n",
    "    \"\"\"\n",
    "    Créer et affiche la matrice de corrélation des attributs liés au dataset.\n",
    "\n",
    "    :param dataset: ensemble des données\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQL8kIMfaVEk"
   },
   "outputs": [],
   "source": [
    "display_corr_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-nwjR6E0W44"
   },
   "source": [
    "#### 4.1.2 Question 11 (5 points)\n",
    "\n",
    "On peut alors choisir de supprimer les attributs qui sont fortement corrélées entre eux en définissant un seuil. Fixons ce seuil à 0.7.\n",
    "\n",
    "**Quels sont les attributs fortement correlés selon le critère ci-dessus ? Supprimez ces attributs et affichez la nouvelle matrice de corrélation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joIHvsMX60pX"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQpercMc60pd"
   },
   "source": [
    "## 5. Entrainement d'un modèle de régression linéaire (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORdHTRsB60pd"
   },
   "source": [
    "### 5.1 Rappel du concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELjDQ7Le60pd"
   },
   "source": [
    "La régression linéaire consiste à trouver une fonction affine qui minimise la somme des carrés des erreurs. La fonction affine est définie par la formule suivante :\n",
    "$$ f(x) = \\beta_0 + \\beta_1^T x $$\n",
    "Nous tentons de trouver les paramètres $\\beta_0$ et $\\beta_1$ qui minimisent $\\sum_{i=1}^n (f(x_i) - y_i)^2=||y-X\\beta||^2$ où $X$ est la matrice des données fournies au modèle et $y$ le vecteur des `SalePrice`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7h64C2F60pd"
   },
   "source": [
    "On veut trouver le minimum de cette fonction. On va utiliser `RidgeRegression` de `sklearn` pour trouver les paramètres $\\beta_0$ et $\\beta_1$. Ce module utilise la méthode des moindres carrés (`numpy.linalg.lstsq`) pour trouver les paramètres $\\beta_0$ et $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wso4UXP260pd"
   },
   "source": [
    "### 5.2 Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY-e46r71ucY"
   },
   "source": [
    "#### 5.2.1 Question 12 (5 points)\n",
    "\n",
    "Après avoir effectué le prétraitement, on peut commencer par séparer les données en un ensemble d'entraînement et un ensemble de test. Pour cela, les 1460 premières lignes contiennent les données d'entrainement. On peut ainsi séparer les données en deux ensembles.\n",
    "\n",
    "**Compléter la structure suivante afin de diviser les données en deux sous-ensembles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMK22chk60pe"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "data_train = {\"x\": pass, \"y\": pass, \"df\": pass}\n",
    "data_pred = {\"x\": pass, \"df\": pass}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl68_rKE60pe"
   },
   "source": [
    "#### 5.2.2 Question 13 (7.5 points)\n",
    "\n",
    "Une fois cette séparation faite, on peut utiliser `RidgeRegression` pour effectuer la régression linéaire avec pénalisation de la norme L2.\n",
    "\n",
    "**Compléter la fonction `ridge_regression`. Elle doit implémenter l'ensemble de la régression.**\n",
    "\n",
    "*Pour cette question, vous devez retourner les coefficients de la regression linéaire. De plus, cette fonction doit modifier le paramètre `data_pred` en y ajoutant les valeurs prédites.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarques**:\n",
    "- La fonction doit retourner un dictionnaire dont les clés sont les noms des attributs et les valeurs, les coefficients correspondants.\n",
    "- Dans `data_pred`, ajoutez deux colonnes : une première contenant les prédictions (sortie du modèle), et une deuxième avec les prédictions remises à l’échelle originale des prix, en inversant la standardisation et la transformation logarithmique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2ZKD-jN60pe"
   },
   "outputs": [],
   "source": [
    "def ridge_regression(data_train, data_pred):\n",
    "    \"\"\"\n",
    "    Réaliser la prédiction selon la régression de Rigde.\n",
    "\n",
    "    :param data_train: données d'entrainement\n",
    "    :param data_pred: données de prédiction\n",
    "    \n",
    "    :return:\n",
    "      coefficients de la régression\n",
    "    \"\"\"\n",
    "    data_pred[\"y\"] = #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmCcvTKU1TQQ"
   },
   "outputs": [],
   "source": [
    " reg = ridge_regression(data_train, data_pred)\n",
    " data_pred[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YqmB6786dJ7"
   },
   "source": [
    "#### 5.2.3 Question 14 (5 points)\n",
    "\n",
    "**À l’aide d’un histogramme, comparez la distribution des prix prédits à celle des données d’entraînement, en vous assurant que les deux sont représentées à l’échelle originale des prix (c’est-à-dire sans transformation logarithmique ni standardisation). Commentez brièvement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFjYYGaz60pf"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be4k6q0-5Siq"
   },
   "source": [
    "### 5.3. Sélection des attributs importants\n",
    "#### 5.3.1 Question 15 (5 points)\n",
    "\n",
    "Une fois la prédiction obtenue, on peut maintenant mesurer l'importance de chaque attribut dans la prédicition en traçant les coefficients de la régression linéaire.\n",
    "\n",
    "**Quels sont les dix attributs ayant le plus d'impact dans la prédiction ?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCGi6vQI60pg"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXVWypfjeahG"
   },
   "source": [
    "#### 5.3.2 Question 16 (7.5 points)\n",
    "\n",
    "Cette dernière méthode n'est pas nécessairement une bonne mesure de l'importance d'un attribut. On peut utiliser la méthode SHAP (SHapley Additive exPlanations) pour effectuer la sélection des attributs.\n",
    "\n",
    "**Les dix attributs ayant le plus d'impact dans la prédiction pour cette mesure sont-ils les mêmes que ceux de la question précédente ? Donnez une interprétation comparative de ces deux résultats**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFBr7mdXgwef"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEkbTIvZ59aP"
   },
   "source": [
    "## 6. Méthode des écarts interquartiles ou IRQ (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egC3g-Pk60pT"
   },
   "source": [
    "On peut également détecter les valeurs aberrantes en affichant un boxplot de chaque colonne. On considère les valeurs comme aberrantes si elles sont situées en dehors de l'intervalle [Q1 - α * IQR, Q3 + α * IQR] où\n",
    "* Q1 et Q3 sont les quantiles 25% et 75%,\n",
    "* IQR l'intervalle interquartile (Q3 - Q1)\n",
    "* α le facteur d'ajustement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette question, on exclut `SalePrice` car les seules valeurs manquantes de cet attribut sont celles du dataset de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important :** Pour cette question, utilisez le jeux de données `df_order1` copié vers la fin de la section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Question 17 (5 points)\n",
    "\n",
    "Testez plusieurs valeurs du facteur d'ajustement α dans l'intervalle [1.5, 5] avec un pas de 0.5.\n",
    "Pour chaque valeur de α :\n",
    "- Calculez les bornes de détection des valeurs aberrantes (fences) pour chaque attribut numérique.\n",
    "- Déterminez le nombre de valeurs aberrantes détectées pour chaque attribut.\n",
    "- Affichez les résultats dans un `DataFrame`, où les lignes correspondent aux différentes valeurs de α et les colonnes correspondent aux attributs numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque :** Vous pouvez utiliser la fonction `percentile` de `numpy` pour calculer les quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y3CRdn2sNH-W"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisez les résultats dans une seule figure:\n",
    "- L’axe X représente les valeurs de α.\n",
    "- L’axe Y représente le nombre de valeurs aberrantes.\n",
    "- Chaque courbe correspond à un attribut numérique, illustrant l’évolution du nombre de valeurs aberrantes en fonction de α.\n",
    "\n",
    "Quelle valeur de α vous semble le mieux adaptée? Justifiez votre réponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wj-3CYhqNGuG"
   },
   "source": [
    "### 6.2 Question 18 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On  fixe le facteur d'ajustement α à `3` pour tous les attributs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque attribut numérique, tracez un boxplot. Ajoutez également deux lignes horizontales représentant les bornes inférieure et supérieure de l’intervalle [Q1 - α * IQR, Q3 + α * IQR]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGf-A4Xz60pT"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque attribut numérique (à l'exception de `BsmtFinSF2`), remplacez les valeurs aberrantes détectées avec α=3 par la `valeur médiane` de la colonne. Pourquoi ce remplacement n’est pas approprié pour l’attribut `BsmtFinSF2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "A2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
