{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfEnYLlo9yAM"
   },
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP1 ETE 2025 - Préparation de données\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aveyr1en994y"
   },
   "source": [
    "### Instructions de remise\n",
    "\n",
    "#### Membres de l'équipe :\n",
    "    - Nom (Matricule) 1\n",
    "    - Nom (Matricule) 2\n",
    "    - Nom (Matricule) 3\n",
    "\n",
    "#### Numéro du groupe :\n",
    "    - TP - Groupe #\n",
    "    \n",
    "#### Livrable :\n",
    "\n",
    "Vous devez soumettre ce notebook sur Moodle dans la boite de remise sous le nom TP1_NumeroGroupe_matricule1_matricule2_matricule3.ipynb.\n",
    "\n",
    "**NB**: Tout travail en retard sera pénalisé d'une valeur de 10\\% par jour de retard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WFBkYc660pI"
   },
   "source": [
    "## Introduction et objectifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fbCQ4YHNFtn"
   },
   "source": [
    "### Importation des différents modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dni6R3K760pK"
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install plotly\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "beJjTQOt60pM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn import linear_model\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe5Ms4DINc8u"
   },
   "source": [
    "### Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m_QcX1llNT2d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txn5hmVWBSzF"
   },
   "source": [
    "Le but de ce notebook est d'effectuer le prétraitement du dataset [HousePricePrediction](https://docs.google.com/spreadsheets/d/1caaR9pT24GNmq3rDQpMiIMJrmiTGarbs/edit#gid=1150341366) qui pourra être par la suite être utilisé pour entraîner un modèle de prédiction de prix de maisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyYKhZfI60pN"
   },
   "source": [
    "## Préparation des données\n",
    "\n",
    "Plusieurs étapes sont nécessaires pour préparer un dataset pour la fouille des données\n",
    "- **Nettoyage des données** :\n",
    "    - Suppression des attributs inutiles\n",
    "    - Gestion des valeurs manquantes\n",
    "    - Gestion des valeurs aberrantes\n",
    "- **Transformation des données** :\n",
    "    - Encodage des données\n",
    "    - Normalisation des données\n",
    "- **Sélection des attributs** :\n",
    "    - Suppression des attributs les plus fortement corrélés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SXXhu9S60pO"
   },
   "source": [
    "<a id=\"exploration-des-donnees\"></a>\n",
    "## 1. Exploration des données (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPtQqe4J60pO"
   },
   "source": [
    "Nous vous avons fourni le fichier *data.csv* avec l'exécution de la deuxième cellule. Il contient l'ensemble des données. Chaque ligne contient les données d'une vente. La description des attributs du dataset est la suivante:\n",
    "\n",
    "| # | Feature Name | Description |\n",
    "|---|--------------|-------------|\n",
    "| 1 | Id           | Numéro de vente / To count the records. |\n",
    "| 2 | MSSubClass   | Type de logement / Identifies the type of dwelling involved in the sale. |\n",
    "| 3 | MSZoning     | Zonage / Identifies the general zoning classification of the sale. |\n",
    "| 4 | LotArea      | Superficie du logement / Lot size in square feet. |\n",
    "| 5 | LotConfig    | Configuration du logement / Configuration of the lot |\n",
    "| 6 | BldgType     | Type de logement / Type of dwelling |\n",
    "| 7 | OverallCond  | Etat général / Rates the overall condition of the house |\n",
    "| 8 | YearBuilt    | Année de contruction / Original construction year |\n",
    "| 9 | YearRemodAdd | Année de rénovation / Remodel date (same as construction date if no remodeling or additions). |\n",
    "| 10| Exterior1st  | Type de revêtement extérieur / Exterior covering on house |\n",
    "| 11| BsmtFinSF2   | Surface de vie / Type 2 finished square feet. |\n",
    "| 12| TotalBsmtSF  | Surface totale de la base / Total square feet of basement area |\n",
    "| 13| SalePrice    | Prix de vente à prédire / To be predicted |\n",
    "\n",
    "On visualise le dataset pour avoir une idée de ce qu'il contient et des prétraitements à effectuer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaalN_p760pO"
   },
   "source": [
    "### 1.1 - Question 1 (2.5 points)\n",
    "\n",
    "**Combien d'éléments contient le dataset ? Quelles sont les types des attributs du dataset ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srCKpbVl60pO"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iopxmVPF60pP"
   },
   "source": [
    "### 1.2 - Question 2 (2.5 points)\n",
    "\n",
    "**Quelles sont les valeurs uniques des attributs de type `object` ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akNtIgGE60pP"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft67cmEY60pQ"
   },
   "source": [
    "<a id=\"nettoyage-des-donnees\"></a>\n",
    "## 2. Nettoyage des données (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEAKIo8W60pQ"
   },
   "source": [
    "<a id=\"suppression-des-attributs-inutiles\"></a>\n",
    "### 2.1 Suppression des attributs inutiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njGJm6PHiiAK"
   },
   "source": [
    "### 2.1.1 - Question 3 (5 points)\n",
    "\n",
    "**Pourquoi on peut supprimer l'attribut `Id` dans le cas de ce TP? Effectuez cette suppression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS6YJCwI60pQ"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWPPDvJx60pQ"
   },
   "source": [
    "<a id=\"gestion-des-valeurs-manquantes\"></a>\n",
    "### 2.2 Gestion des valeurs manquantes\n",
    "\n",
    "Pour gérer les valeurs manquantes, plusieurs solutions s'offrent à nous :\n",
    "- Remplacer par la valeur la plus fréquente (le mode)\n",
    "- Remplacer par la valeur moyenne/médiane\n",
    "- Suppression des lignes contenant des valeurs manquantes\n",
    "\n",
    "Pour ce TP, nous utiliserons la dernière option car nous avons peu de valeurs manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijKA36QIhN9O"
   },
   "source": [
    "#### 2.2.1 - Question 4 (2.5 points)\n",
    "\n",
    "**Quels attributs contiennent des valeurs manquantes ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPdUgLoC60pQ"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD3p7SzPhvJr"
   },
   "source": [
    "#### 2.2.2 - Question 5 (2.5 points)\n",
    "\n",
    "On peut alors gérer les valeurs manquantes colonne par colonne. L'attribut `SalePrice` n'est pas pris en considération car les valeurs manquantes sont justement les valeurs que nous voulons prédire.\n",
    "\n",
    "**Supprimer les lignes contenant les valeurs manquantes. Implémentez la fonction `delete_missing_values` qui retire ces données**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lB8c_BHB60pS"
   },
   "outputs": [],
   "source": [
    "def delete_missing_values(dataset):\n",
    "    \"\"\"\n",
    "    This function deletes row whom a value is missing.\n",
    "\n",
    "    :param dataset: ensemble des données\n",
    "    :return:\n",
    "      dataset traitée\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOQyhy5Bzeea"
   },
   "outputs": [],
   "source": [
    "df = delete_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv7C76Pt60pR"
   },
   "source": [
    "Les données manquantes pour la colonne `SalePrice` sont celles du dataset de test. On laisse donc ces valeurs manquantes car on veut appliquer le même prétraitement sur les données de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd7i82HySOTw"
   },
   "source": [
    "### 2.2.3 - Question 6 (10 points)\n",
    "\n",
    "On veut néanmoins que les données d'entrainement suivent une distribution gaussienne.\n",
    "\n",
    "**Implémenter le fonction `plot_hist`. Cette fonction doit permettre d'afficher la distribution des valeurs de l'attribut `SalePrice` ainsi que la loi normale de même moyenne et variance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJVGJ9rm60pR"
   },
   "outputs": [],
   "source": [
    "def plot_hist(prices):\n",
    "    \"\"\"\n",
    "    Affiche la distribution du prix de vente\n",
    "\n",
    "    :param prices: ensemble des prix.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouDFr8tK60pR"
   },
   "outputs": [],
   "source": [
    "plot_hist(df['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niiWCCwX60pS"
   },
   "source": [
    "Vous devez obtenir une distribution des valeurs de `SalePrice` proches d'une distribution normale mais légèrement asymétrique. On peut alors appliquer une transformation logarithmique pour approcher d'une distribution normale symétrique.\n",
    "\n",
    "**Effectuer cette transformation sur notre ensemble de données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDakO5Is60pS"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVO6QYKK_msM"
   },
   "outputs": [],
   "source": [
    "plot_hist(df['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette copie va servir plus tard pour la question 6 (IQR)\n",
    "df_order1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv5pOxkH60pT"
   },
   "source": [
    "<a id=\"detection-des-valeurs-aberrantes\"></a>\n",
    "### 2.3 Détection des valeurs aberrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96toDgSp60pT"
   },
   "source": [
    "En pratique, la méthode de détection d'une valeur aberrante nécessite de se poser les questions suivantes:\n",
    "- Quelles valeurs seraient incohérentes pour chaque colonne ?\n",
    "- Quelles valeurs peuvent être problématiques pour l'utilisation de ces données ? Exemple: pour une régression linéaire, on préfère avoir des valeurs distribuées suivant une loi normale.\n",
    "\n",
    "Avec ces éléments, on peut:\n",
    "- Fixer des seuils de tolérance pour les valeurs aberrantes\n",
    "- Utiliser des algorithmes de détection de valeurs aberrantes (ex: clustering, IRQ, [QTest](https://plotly.com/python/v3/outlier-test/), ...)\n",
    "\n",
    "A noter que suivant les méthodes, les valeurs détectées comme aberrantes peuvent être différentes.\n",
    "\n",
    "La méthode IRQ fait l'objet d'une question, en fin de ce notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwgcO0k9Jjiw"
   },
   "source": [
    "### 2.3.1 Question 7 (10 points)\n",
    "\n",
    "Ici comme nous allons réaliser une régression linéaire, nous allons visuellement voir si certains points s'écartent largement de la droite de régression.\n",
    "\n",
    "On sait que l'on veut effectuer une régression linéaire pour prédire `SalePrice`. On peut donc visualiser les valeurs de chaque attribut en fonction de `SalePrice` pour détecter la présence de valeurs aberrantes.\n",
    "\n",
    "**Implémenter la fonction `plot_line`. Elle doit permettre de visualiser la relation entre un attribut donné et `SalePrice`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGLszIizKSYO"
   },
   "outputs": [],
   "source": [
    "def plot_line(attr):\n",
    "    \"\"\"\n",
    "    Affiche la relation entre attr et SalePrice\n",
    "\n",
    "    :param attr: attribut à comparer à SalePrice\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq_0y4OeK_Pm"
   },
   "source": [
    "**Afficher les relations de tous les attributs avec `SalePrice`. Peut-on y déceler des valeurs aberrantes ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EPDHuUJZ1a4"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À ce stade, il s'agit uniquement de détecter la présence éventuelle de valeurs aberrantes dans les données.**\n",
    "**Aucune action de traitement ou de remplacement n'est demandée pour le moment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrnapsJb60pV"
   },
   "source": [
    "## 3. Transformation des données (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZKsU-2t60pV"
   },
   "source": [
    "### 3.1 Encodage des attributs de type `object`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MElC6Gao60pV"
   },
   "source": [
    "Les attributs de type `object` étant catégoriques (voire partie 1), on peut effectuer un `one hot encoding` de ces attributs. `Pandas` permet d'effectuer cela avec la fonction `get_dummies()`. Cela nous permettra d'obtenir un dataset contenant uniquement des attributs de type `int` ou `float`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHcKIIn_xcbg"
   },
   "source": [
    "#### 3.1.1 Question 8 (5 points)\n",
    "\n",
    "**Encodez les attributs de type `object` avec un `one hot encoding`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oTBcJTT60pV"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nern4lpg60pW"
   },
   "source": [
    "### 3.2 Normalisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WharCiE560pW"
   },
   "source": [
    "Pour faciliter l'entraînement du modèle, on peut normaliser les données. `sklearn` permet d'effectuer cela avec les fonctions suivantes :\n",
    "\n",
    "*   `StandardScaler()` normalise les données en soustrayant la moyenne et en divisant par l'écart-type\n",
    "*   `MinMaxScaler()` normalise les données en les ramenant entre 0 et 1.\n",
    "\n",
    "Dans la suite de ce TP, nous utiliserons la fonction `StandardScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGl8KxkwXyC2"
   },
   "outputs": [],
   "source": [
    "# A utiliser dans la partie 5.2\n",
    "mu_sale_price = df[\"SalePrice\"].mean()\n",
    "sigma_sale_price = df[\"SalePrice\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0_AYgbsxzdY"
   },
   "source": [
    "#### 3.2.1 Question 9 (5 points)\n",
    "\n",
    "**Implémenter la fonction `normalize`. Elle doit réaliser la normalisation des données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsJz5irG60pW"
   },
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    \"\"\"\n",
    "    Normalise les données du dataset.\n",
    "\n",
    "    :param dataset: ensemble des données\n",
    "    :return:\n",
    "      dataset traitée\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2amj4-7mcPjS"
   },
   "outputs": [],
   "source": [
    "df = normalize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0AY3k-a60pX"
   },
   "source": [
    "## 4. Sélection des attributs corrélées (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QdxBuVE60pX"
   },
   "source": [
    "### 4.1 Suppression des attributs corrélées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqQZacRb60pX"
   },
   "source": [
    "Pour améliorer la qualité de la prédiction, nous devons prendre en compte la corrélation entre attributs. L'objectif est donc de supprimer les attributs les plus fortement corrélées entre eux.\n",
    "\n",
    "Pour ce faire, vous disposez des fonctions suivantes\n",
    "\n",
    "* `corr()` de `Pandas` qui calcule la matrice de corrélation\n",
    "* `heatmap()` de `seaborn` qui permet de visualiser la matrice de corrélation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_vrfq-hyJdL"
   },
   "source": [
    "#### 4.1.1 Question 10 (10 points)\n",
    "\n",
    "**Implémenter la fonction `display_corr_matrix`. Elle doit permettre d'afficher la matrice de corrélation entre les différents attributs de nos données après normalisation des données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAM9uYXW60pX"
   },
   "outputs": [],
   "source": [
    "def display_corr_matrix(dataset)\n",
    "    \"\"\"\n",
    "    Créer et affiche la matrice de corrélation des attributs liés au dataset.\n",
    "\n",
    "    :param dataset: ensemble des données\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQL8kIMfaVEk"
   },
   "outputs": [],
   "source": [
    "display_corr_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-nwjR6E0W44"
   },
   "source": [
    "#### 4.1.2 Question 11 (5 points)\n",
    "\n",
    "On peut alors choisir de supprimer les attributs qui sont fortement corrélées entre eux en définissant un seuil. Fixons ce seuil à 0.7.\n",
    "\n",
    "**Quels sont les attributs fortement correlés selon le critère ci-dessus ? Supprimez ces attributs et affichez la nouvelle matrice de corrélation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joIHvsMX60pX"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQpercMc60pd"
   },
   "source": [
    "## 5. Entrainement d'un modèle de régression linéaire (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORdHTRsB60pd"
   },
   "source": [
    "### 5.1 Rappel du concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELjDQ7Le60pd"
   },
   "source": [
    "La régression linéaire consiste à trouver une fonction affine qui minimise la somme des carrés des erreurs. La fonction affine est définie par la formule suivante :\n",
    "$$ f(x) = \\beta_0 + \\beta_1^T x $$\n",
    "Nous tentons de trouver les paramètres $\\beta_0$ et $\\beta_1$ qui minimisent $\\sum_{i=1}^n (f(x_i) - y_i)^2=||y-X\\beta||^2$ où $X$ est la matrice des données fournies au modèle et $y$ le vecteur des `SalePrice`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7h64C2F60pd"
   },
   "source": [
    "On veut trouver le minimum de cette fonction. On va utiliser `RidgeRegression` de `sklearn` pour trouver les paramètres $\\beta_0$ et $\\beta_1$. Ce module utilise la méthode des moindres carrés (`numpy.linalg.lstsq`) pour trouver les paramètres $\\beta_0$ et $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wso4UXP260pd"
   },
   "source": [
    "### 5.2 Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY-e46r71ucY"
   },
   "source": [
    "#### 5.2.1 Question 12 (5 points)\n",
    "\n",
    "Après avoir effectué le prétraitement, on peut commencer par séparer les données en un ensemble d'entraînement et un ensemble de test. Pour cela, les 1460 premières lignes contiennent les données d'entrainement. On peut ainsi séparer les données en deux ensembles.\n",
    "\n",
    "**Compléter la structure suivante afin de diviser les données en deux sous-ensembles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMK22chk60pe"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "data_train = {\"x\": pass, \"y\": pass, \"df\": pass}\n",
    "data_pred = {\"x\": pass, \"df\": pass}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl68_rKE60pe"
   },
   "source": [
    "#### 5.2.2 Question 13 (7.5 points)\n",
    "\n",
    "Une fois cette séparation faite, on peut utiliser `RidgeRegression` pour effectuer la régression linéaire avec pénalisation de la norme L2.\n",
    "\n",
    "**Compléter la fonction `ridge_regression`. Elle doit implémenter l'ensemble de la régression.**\n",
    "\n",
    "*Pour cette question, vous devez retourner les coefficients de la regression linéaire. De plus, cette fonction doit modifier le paramètre `data_pred` en y ajoutant les valeurs prédites.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarques**:\n",
    "- La fonction doit retourner un dictionnaire dont les clés sont les noms des attributs et les valeurs, les coefficients correspondants.\n",
    "- Dans `data_pred`, ajoutez deux colonnes : une première contenant les prédictions (sortie du modèle), et une deuxième avec les prédictions remises à l’échelle originale des prix, en inversant la standardisation et la transformation logarithmique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2ZKD-jN60pe"
   },
   "outputs": [],
   "source": [
    "def ridge_regression(data_train, data_pred):\n",
    "    \"\"\"\n",
    "    Réaliser la prédiction selon la régression de Rigde.\n",
    "\n",
    "    :param data_train: données d'entrainement\n",
    "    :param data_pred: données de prédiction\n",
    "    \n",
    "    :return:\n",
    "      coefficients de la régression\n",
    "    \"\"\"\n",
    "    data_pred[\"y\"] = #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmCcvTKU1TQQ"
   },
   "outputs": [],
   "source": [
    " reg = ridge_regression(data_train, data_pred)\n",
    " data_pred[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YqmB6786dJ7"
   },
   "source": [
    "#### 5.2.3 Question 14 (5 points)\n",
    "\n",
    "**À l’aide d’un histogramme, comparez la distribution des prix prédits à celle des données d’entraînement, en vous assurant que les deux sont représentées à l’échelle originale des prix (c’est-à-dire sans transformation logarithmique ni standardisation). Commentez brièvement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFjYYGaz60pf"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be4k6q0-5Siq"
   },
   "source": [
    "### 5.3. Sélection des attributs importants\n",
    "#### 5.3.1 Question 15 (5 points)\n",
    "\n",
    "Une fois la prédiction obtenue, on peut maintenant mesurer l'importance de chaque attribut dans la prédicition en traçant les coefficients de la régression linéaire.\n",
    "\n",
    "**Quels sont les dix attributs ayant le plus d'impact dans la prédiction ?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCGi6vQI60pg"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXVWypfjeahG"
   },
   "source": [
    "#### 5.3.2 Question 16 (7.5 points)\n",
    "\n",
    "Cette dernière méthode n'est pas nécessairement une bonne mesure de l'importance d'un attribut. On peut utiliser la méthode SHAP (SHapley Additive exPlanations) pour effectuer la sélection des attributs.\n",
    "\n",
    "**Les dix attributs ayant le plus d'impact dans la prédiction pour cette mesure sont-ils les mêmes que ceux de la question précédente ? Donnez une interprétation comparative de ces deux résultats**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFBr7mdXgwef"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEkbTIvZ59aP"
   },
   "source": [
    "## 6. Méthode des écarts interquartiles ou IRQ (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egC3g-Pk60pT"
   },
   "source": [
    "On peut également détecter les valeurs aberrantes en affichant un boxplot de chaque colonne. On considère les valeurs comme aberrantes si elles sont situées en dehors de l'intervalle [Q1 - α * IQR, Q3 + α * IQR] où\n",
    "* Q1 et Q3 sont les quantiles 25% et 75%,\n",
    "* IQR l'intervalle interquartile (Q3 - Q1)\n",
    "* α le facteur d'ajustement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette question, on exclut `SalePrice` car les seules valeurs manquantes de cet attribut sont celles du dataset de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important :** Pour cette question, utilisez le jeux de données `df_order1` copié vers la fin de la section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Question 17 (5 points)\n",
    "\n",
    "Testez plusieurs valeurs du facteur d'ajustement α dans l'intervalle [1.5, 5] avec un pas de 0.5.\n",
    "Pour chaque valeur de α :\n",
    "- Calculez les bornes de détection des valeurs aberrantes (fences) pour chaque attribut numérique.\n",
    "- Déterminez le nombre de valeurs aberrantes détectées pour chaque attribut.\n",
    "- Affichez les résultats dans un `DataFrame`, où les lignes correspondent aux différentes valeurs de α et les colonnes correspondent aux attributs numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque :** Vous pouvez utiliser la fonction `percentile` de `numpy` pour calculer les quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y3CRdn2sNH-W"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisez les résultats dans une seule figure:\n",
    "- L’axe X représente les valeurs de α.\n",
    "- L’axe Y représente le nombre de valeurs aberrantes.\n",
    "- Chaque courbe correspond à un attribut numérique, illustrant l’évolution du nombre de valeurs aberrantes en fonction de α.\n",
    "\n",
    "Quelle valeur de α vous semble le mieux adaptée? Justifiez votre réponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wj-3CYhqNGuG"
   },
   "source": [
    "### 6.2 Question 18 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On  fixe le facteur d'ajustement α à `3` pour tous les attributs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque attribut numérique, tracez un boxplot. Ajoutez également deux lignes horizontales représentant les bornes inférieure et supérieure de l’intervalle [Q1 - α * IQR, Q3 + α * IQR]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGf-A4Xz60pT"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque attribut numérique (à l'exception de `BsmtFinSF2`), remplacez les valeurs aberrantes détectées avec α=3 par la `valeur médiane` de la colonne. Pourquoi ce remplacement n’est pas approprié pour l’attribut `BsmtFinSF2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
